# ğŸ¤– Jarvis AI Assistant

Jarvis AI Assistant is a modular, self-hosted conversational AI application built using modern backend architecture.  
It integrates a FastAPI backend, a vector database for contextual memory, and a Streamlit-based user interface.  
The system is designed to be LLM-ready and can work with self-hosted models such as LLaMA via Ollama.



## ğŸ“Œ Features

- Modular backend architecture (industry-style)
- Context-aware responses using vector memory
- Pinecone integration for semantic search (optional)
- Self-hosted AI model support (Ollama + LLaMA)
- Interactive chatbot UI using Streamlit
- Easy to extend and scale



## ğŸ› ï¸ Tech Stack

- Backend: FastAPI
- Frontend: Streamlit
- AI / LLM: Local AI engine (LLM-ready, Ollama supported)
- Vector Database: Pinecone
- Embeddings: Sentence Transformers
- Language: Python
- Version Control: Git & GitHub



## ğŸ“‚ Project Structure

```text
jarvis-ai-assistant/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â””â”€â”€ chat_api.py
â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â””â”€â”€ chat_service.py
â”‚ â”œâ”€â”€ ai/
â”‚ â”‚ â”œâ”€â”€ local_llm.py
â”‚ â”‚ â””â”€â”€ ollama_llm.py
â”‚ â”œâ”€â”€ embeddings/
â”‚ â”‚ â””â”€â”€ embedder.py
â”‚ â”œâ”€â”€ vector_store/
â”‚ â”‚ â””â”€â”€ pinecone_store.py
â”‚ â”œâ”€â”€ core/
â”‚ â”‚ â””â”€â”€ config.py
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ui/
â”‚ â”œâ”€â”€ app.py
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ README.md
â””â”€â”€ run_project.md
```


OUTPUT:
<img width="1861" height="864" alt="image" src="https://github.com/user-attachments/assets/9ceb77c6-7ebf-4466-bac9-334ece9a9937" />





